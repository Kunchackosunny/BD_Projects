# -*- coding: utf-8 -*-
"""NLP_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QXUjhp9yj8ZwIH4B7BFHmkaUJ7IB7Bvu
"""

import numpy as np
import pandas as pd
import nltk # for NLP
import re  #for regular expression
import matplotlib.pyplot as plt
import seaborn as sns
#define encoding while doing NLP since thereis a lot of emojis or objects
df=pd.read_csv("/content/twitter_validation.csv",encoding="ISO-8859-1",header=None)
df.columns=['ID','Social_Media','target','text']

df

df.isna().sum()

df.dtypes

df['target'].value_counts()

sns.countplot(x=df['target'],data=df)

df.drop(df.index[(df['target']=="Irrelevant")],axis=0,inplace=True) # removing irrelevant data
df

#making the index no correctly after removing the row
df.reset_index(drop=True,inplace=True)
df

df.drop(['ID','Social_Media'],axis=1,inplace=True)
df

#replacing positive with 1 nuetral with 0 negative with -1
df['target']=df['target'].map({'Positive':1,'Negative':-1,'Neutral':0})
df

tweets=df.text #assigning text to tweets
tweets

nltk.download('stopwords') # to download stopwords
nltk.download('punkt')  #submodule requerd for tokenizingg. we need to download it
nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.tokenize import TweetTokenizer  #word tokenize is also fine
tk=TweetTokenizer()
tweets=tweets.apply(lambda x:tk.tokenize(x)).apply(lambda x:" ".join(x)) #x: " "-- to put space in btwn tokens instead of comma

tweets=tweets.str.replace('[^A-Za-z0-9]+',' ')# sometimes special charecter comes as a continuous format like ###@@@$$$ so we use + to remove those also
tweets

#if the word length is less than 3 we need to remove it. at east more than 3 is needed to get a meaningful word
from nltk.tokenize import word_tokenize
tweets=tweets.apply(lambda x:' '.join([w for w in word_tokenize(x) if len(w)>=3]))
tweets

from nltk.stem import SnowballStemmer
ss=SnowballStemmer('english')
tweets=tweets.apply(lambda x:[ss.stem(i.lower()) for i in tk.tokenize(x)]).apply(lambda x:" ".join(x))
tweets

from nltk.corpus import stopwords
sw=stopwords.words('english')
tweets=tweets.apply(lambda x:[i for i in tk.tokenize(x) if i not in sw ]).apply(lambda x:" ".join(x))
tweets

#vectorization
from sklearn.feature_extraction.text import TfidfVectorizer
vec=TfidfVectorizer()
train_data=vec.fit_transform(tweets)

train_data #this is the x value

x=train_data

y=df['target'].values
y

#now we need to impliment supervised machine learning to predict.
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=42)
x_train

x_test

y_train

y_test

# we can skip normalization step since every data is very small number here. After doing NLP every data will be small

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix,accuracy_score
from sklearn.tree import DecisionTreeClassifier
DecisionTree=DecisionTreeClassifier(criterion='entropy')
knn=KNeighborsClassifier(n_neighbors=7)
base=BernoulliNB()
svm_model=SVC()
lst=[knn,base,svm_model,DecisionTree]

for i in lst:
  print("model name is ,",i)
  i.fit(x_train,y_train)
  print("prediction value is ...")
  y_pred=i.predict(x_test)
  print(y_pred)
  print("confusion matrix is")
  cmd=confusion_matrix(y_test,y_pred)
  print(cmd)
  print("accuracy score is...")
  print(accuracy_score(y_test,y_pred))